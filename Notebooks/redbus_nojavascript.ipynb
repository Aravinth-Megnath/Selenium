{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this code, i have used selenium action chains to move to next pages-1,2,3,4,5. \n",
    "#Page_number variable to store current page number\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 30)  # Increased timeout\n",
    "\n",
    "driver.get(\"https://www.redbus.in/online-booking/apsrtc/?utm_source=rtchometile\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "def scrape_page():\n",
    "    # Locate elements\n",
    "    routes = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"route_link\")))\n",
    "\n",
    "    # Loop through each route to extract details\n",
    "    for route in routes:\n",
    "        try:\n",
    "            route_link = route.find_element(By.CSS_SELECTOR, \".route_details a\").get_attribute(\"href\")\n",
    "            title = route.find_element(By.CSS_SELECTOR, \".route_details a\").get_attribute(\"title\")\n",
    "            start_price = route.find_element(By.CSS_SELECTOR, \".route_details .fare\").text.strip()\n",
    "            bus_options = route.find_element(By.CSS_SELECTOR, \".row2 .totalRoutes:nth-of-type(1)\").text.strip()\n",
    "            first_bus = route.find_element(By.CSS_SELECTOR, \".row2 .totalRoutes:nth-of-type(2) strong\").text.strip()\n",
    "            last_bus = route.find_element(By.CSS_SELECTOR, \".row2 .totalRoutes:nth-of-type(3) strong\").text.strip()\n",
    "\n",
    "            # Append extracted data to list\n",
    "            all_data.append({\n",
    "                \"route_link\": route_link,\n",
    "                \"title\": title,\n",
    "                \"start_price\": start_price,\n",
    "                \"bus_options\": bus_options,\n",
    "                \"first_bus\": first_bus,\n",
    "                \"last_bus\": last_bus\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "# Scrape data from the first 5 pages\n",
    "for page_number in range(1, 6):\n",
    "    scrape_page()\n",
    "    if page_number < 5:  # Don't try to click next on the last page\n",
    "        try:\n",
    "            # Locate the pagination container\n",
    "            pagination_container = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, '//*[@id=\"root\"]/div/div[4]/div[12]')\n",
    "            ))\n",
    "\n",
    "            # Locate the next page button within the container\n",
    "            next_page_button = pagination_container.find_element(\n",
    "                By.XPATH, f'.//div[contains(@class, \"DC_117_pageTabs\") and text()=\"{page_number + 1}\"]'\n",
    "            )\n",
    "            \n",
    "            # Ensure the next page button is in view\n",
    "            actions = ActionChains(driver)\n",
    "            actions.move_to_element(next_page_button).perform()\n",
    "            time.sleep(1)  # Wait for a bit after scrolling\n",
    "            \n",
    "            # Log the action\n",
    "            print(f\"Clicking on page {page_number + 1}\")\n",
    "            \n",
    "            # Click the next page button\n",
    "            next_page_button.click()\n",
    "            \n",
    "            # Wait for the page number to update to the next page\n",
    "            wait.until(EC.text_to_be_present_in_element(\n",
    "                (By.XPATH, '//div[contains(@class, \"DC_117_pageTabs DC_117_pageActive\")]'), str(page_number + 1)))\n",
    "            \n",
    "            # Log the successful page navigation\n",
    "            print(f\"Successfully navigated to page {page_number + 1}\")\n",
    "            \n",
    "            # Wait for a short duration to ensure the next page loads completely\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while navigating to page {page_number + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "# Print the scraped data\n",
    "for entry in all_data:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd way to scroll down the page using driver.page_source option using selenium\n",
    "#its a built in option offered by selenium\n",
    "#Check the Scroll and collect function to understand how it works\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "# Assuming all_data is defined somewhere in your script\n",
    "route_links = [entry['route_link'] for entry in all_data]\n",
    "\n",
    "bus_data = []\n",
    "\n",
    "def scroll_and_collect():\n",
    "    body = driver.find_element(By.TAG_NAME, 'body')\n",
    "    scrolling = True\n",
    "    \n",
    "    while scrolling:\n",
    "        old_page_source = driver.page_source\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for the page to load new content\n",
    "        new_page_source = driver.page_source\n",
    "\n",
    "        if new_page_source == old_page_source:\n",
    "            scrolling = False\n",
    "    \n",
    "    # Get bus items after scrolling\n",
    "    bus_items_after_scroll = driver.find_elements(By.CLASS_NAME, \"bus-item\")\n",
    "\n",
    "def scrape_bus_data(route_link):\n",
    "    driver.get(route_link)\n",
    "    wait = WebDriverWait(driver, 30)  # Increase timeout to 30 seconds\n",
    "\n",
    "    try:\n",
    "        # Find all \"View Buses\" buttons and click them\n",
    "        \n",
    "        view_buses_buttons = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \"//div[contains(@class, 'button') and contains(text(), 'View Buses')]\")))\n",
    "\n",
    "        for button in view_buses_buttons:\n",
    "            try:\n",
    "                button.click()\n",
    "                time.sleep(3)  # Wait for the content to load\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while clicking the button: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Collect bus details\n",
    "        scroll_and_collect()\n",
    "        bus_items = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"bus-item\")))\n",
    "        for bus in bus_items:\n",
    "            try:\n",
    "                busname = bus.find_element(By.CLASS_NAME, \"travels\").text\n",
    "                bustype = bus.find_element(By.CLASS_NAME, \"bus-type\").text\n",
    "                departing_time = bus.find_element(By.CLASS_NAME, \"dp-time\").text\n",
    "                duration = bus.find_element(By.CLASS_NAME, \"dur\").text\n",
    "                reaching_time = bus.find_element(By.CLASS_NAME, \"bp-time\").text\n",
    "                star_rating = bus.find_element(By.CSS_SELECTOR, \".rating-sec .rating span\").text\n",
    "                price = bus.find_element(By.CSS_SELECTOR, \".seat-fare .fare span\").text\n",
    "                seats_available = bus.find_element(By.CLASS_NAME, \"seat-left\").text\n",
    "\n",
    "                bus_data.append({\n",
    "                    \"route_link\": route_link,\n",
    "                    \"busname\": busname,\n",
    "                    \"bustype\": bustype,\n",
    "                    \"departing_time\": departing_time,\n",
    "                    \"duration\": duration,\n",
    "                    \"reaching_time\": reaching_time,\n",
    "                    \"star_rating\": star_rating,\n",
    "                    \"price\": price,\n",
    "                    \"seats_available\": seats_available\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while scraping bus data: {e}\")\n",
    "                continue\n",
    "    except TimeoutException as e:\n",
    "        print(f\"Timeout occurred while waiting for elements: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "\n",
    "# Scrape data for each route link\n",
    "for route_link in route_links:\n",
    "    scrape_bus_data(route_link)\n",
    "    time.sleep(4)\n",
    "\n",
    "# Print the scraped bus data\n",
    "for entry in bus_data:\n",
    "    print(entry)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c703d9b332acc10209d52dc6dfb15bb88c7701d2a1dd8dfc007f8dd382943fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
